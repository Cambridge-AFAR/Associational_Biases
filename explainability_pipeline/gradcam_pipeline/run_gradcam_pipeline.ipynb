{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9692a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import cv2\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd527ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_path)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def reshape_transform(tensor):\n",
    "    B, N, C = tensor.shape\n",
    "    H_W = int((N - 1) ** 0.5)\n",
    "    assert H_W * H_W == (N - 1), f\"Cannot reshape {N - 1} tokens into square feature map\"\n",
    "\n",
    "    tensor = tensor[:, 1:, :]\n",
    "    return tensor.reshape(B, H_W, H_W, C).permute(0, 3, 1, 2)\n",
    "\n",
    "class TokenLogitWrapper(torch.nn.Module):\n",
    "    def __init__(self, base_model, inputs_template, token_index, num_image_views=5):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.base_inputs = inputs_template.copy()\n",
    "        self.base_inputs.pop(\"pixel_values\", None)\n",
    "        self.base_inputs.pop(\"image_sizes\", None)\n",
    "        self.token_index = token_index\n",
    "        self.num_image_views = num_image_views\n",
    "\n",
    "    def forward(self, pixel_values_4d):\n",
    "        current_inputs = self.base_inputs.copy()\n",
    "        B, C, H, W = pixel_values_4d.shape\n",
    "        pixel_values_5d = torch.zeros(  \n",
    "                                        B, \n",
    "                                        self.num_image_views, \n",
    "                                        C, \n",
    "                                        H, \n",
    "                                        W,\n",
    "                                        dtype = pixel_values_4d.dtype,\n",
    "                                        device = pixel_values_4d.device\n",
    "                                    )\n",
    "        pixel_values_5d[:, 0, :, :, :] = pixel_values_4d\n",
    "\n",
    "        current_inputs[\"pixel_values\"] = pixel_values_5d\n",
    "        current_inputs[\"image_sizes\"] = torch.tensor([(H, W)] * B, device = pixel_values_4d.device)\n",
    "\n",
    "        out = self.model(**current_inputs)\n",
    "        logits = out.logits\n",
    "\n",
    "        # shape: [batch_size, vocab_size]\n",
    "        target_logits = logits[:, self.token_index, :]\n",
    "        return target_logits.unsqueeze(0) if target_logits.ndim == 1 else target_logits\n",
    "\n",
    "def upscale_image_if_needed(img, target_size=336):\n",
    "    if min(img.size) < target_size:\n",
    "        return img.resize((target_size, target_size), Image.BICUBIC)\n",
    "    return img\n",
    "\n",
    "def rect_to_mask(image_shape, rect):\n",
    "    x1, y1, x2, y2 = map(int, rect)\n",
    "    mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "    mask[y1 : y2, x1 : x2] = 1\n",
    "    return mask\n",
    "\n",
    "def scale_bbox(bbox, orig_size, new_size):\n",
    "    x_scale = new_size[0] / orig_size[0]\n",
    "    y_scale = new_size[1] / orig_size[1]\n",
    "    x, y, w, h = bbox\n",
    "    return [int(x * x_scale), int(y * y_scale), int(w * x_scale), int(h * y_scale)]\n",
    "\n",
    "def average_activation(cam_map: np.ndarray, region_mask: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes average GradCAM activation within the region defined by region_mask.\n",
    "    \"\"\"\n",
    "    region_area = np.sum(region_mask)\n",
    "    if region_area == 0:\n",
    "        return 0.0\n",
    "    return np.sum(cam_map * region_mask) / region_area\n",
    "\n",
    "def save_batch_to_csv(log_data, output_path, fieldnames, append=True):\n",
    "    mode = 'a' if append else 'w'\n",
    "    write_header = not os.path.exists(output_path) or not append\n",
    "\n",
    "    with open(output_path, mode=mode, newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(log_data)\n",
    "    print(f\"Saved {len(log_data)} entries to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/content/drive/MyDrive/raf_anger_images\"\n",
    "root_output_dir = root_dir + \"/anger_activation_results\"\n",
    "output_csv_filename = \"activation_results.csv\"\n",
    "output_csv_path = os.path.join(root_output_dir, output_csv_filename)\n",
    "log_file_path = os.path.join(root_output_dir, \"processed_images.txt\")\n",
    "image_dir = root_dir + \"/all_images\"\n",
    "annotation_json_path = root_dir + \"anger_all_filtered_faces_and_hair.json\"\n",
    "hair_mask_dir = os.path.join(root_dir + \"/hair_segmentation\", \"hair_masks_fullsize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00cafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"anger\", \"angry\"]\n",
    "\n",
    "# emotion prompt text\n",
    "prompt_txt = (\n",
    "    \"<image>\\nFocusing only on the expressions and emotions of the person or people shown, describe the emotions and expressions of the person or people in the image from one of the following emotions: [happiness, sadness, fear, anger, neutral, unsure]. Keep the description to at most 50-60 words.\"\n",
    ")\n",
    "\n",
    "\n",
    "# activity prompt text\n",
    "# prompt_txt = (\n",
    "#     \"<image>\\nFocusing only on the activities that the person or people shown are doing, describe the activities that the person or people shown are doing from one of the following categories of activities: [helping and caring, eating, household, dance and music, personal care, posing, sports, transportation, work, other, unsure]. Keep the description to at most 50-60 words.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_log = []\n",
    "csv_fieldnames = [\"image\", \"token_index\", \"token\", \"face_id\", \"gender\", \"hair\", \"face\", \"background\", \"non_background\"]\n",
    "\n",
    "with open(annotation_json_path, \"r\") as f:\n",
    "    annotation_data = json.load(f)\n",
    "\n",
    "processed_images_set = set()\n",
    "if os.path.exists(log_file_path):\n",
    "    try:\n",
    "        with open(log_file_path, mode='r') as f:\n",
    "            processed_images_set = {line.strip() for line in f}\n",
    "        print(f\"Resuming run. Found {len(processed_images_set)} images already processed in the log file.\")\n",
    "    except (IOError, ValueError) as e:\n",
    "        print(f\"Warning: Could not read existing log file at {log_file_path}. Starting from scratch. Error: {e}\")\n",
    "\n",
    "# get a list of already processed images in case a previous run was interrupted\n",
    "all_image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith((\".png\", \".jpg\"))]\n",
    "remaining_image_paths = [path for path in all_image_paths if os.path.splitext(os.path.basename(path))[0] not in processed_images_set]\n",
    "\n",
    "try:\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        # write the list of already-processed files to the log\n",
    "        for image_name in processed_images_set:\n",
    "            f.write(f\"{image_name}\\n\")\n",
    "    print(f\"Log file initialized with {len(processed_images_set)} previously processed images.\")\n",
    "except IOError as e:\n",
    "    print(f\"Error writing to initial log file: {e}\")\n",
    "\n",
    "print(f\"Total images to process in this run: {len(remaining_image_paths)}\")\n",
    "\n",
    "total_identified_images = 0\n",
    "total_unidentified_images = 0\n",
    "processed_image_count = 0\n",
    "SAVE_INTERVAL = 25\n",
    "\n",
    "for image_path in tqdm(remaining_image_paths, desc = \"Processing Images\"):\n",
    "\n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    image_output_dir = os.path.join(root_output_dir + \"/image_results\", image_name + \"_test\")\n",
    "\n",
    "    try:\n",
    "        with open(log_file_path, 'a') as f:\n",
    "            f.write(f\"{image_name}\\n\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error appending to log file for image {image_name}: {e}\")\n",
    "\n",
    "    if image_name not in annotation_data:\n",
    "        print(\"Continuing because image is not in annotation data\")\n",
    "        continue\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    orig_size = image.size      # store original size before potential upscale\n",
    "    image = upscale_image_if_needed(image)      # ensure image is at least target_size\n",
    "\n",
    "    # prepare inputs using the processor\n",
    "    inputs = processor(prompt_txt, image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    pixel_values = inputs[\"pixel_values\"].clone().detach().requires_grad_(True)\n",
    "\n",
    "    # the pixel_values tensor from LLaVA's processor will be [1, num_views, C, H, W]\n",
    "    pixel_values_for_gradcam = inputs[\"pixel_values\"][:, 0, :, :, :].clone().detach().requires_grad_(True)\n",
    "\n",
    "    # get the number of image views expected by the model for the TokenLogitWrapper\n",
    "    # this is the second dimension of the original pixel_values\n",
    "    num_image_views_in_input = inputs[\"pixel_values\"].shape[1]\n",
    "\n",
    "    # generate output\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=120)\n",
    "\n",
    "    caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    assistant_response = caption.split(\"\\n\\n\")[-1].strip()\n",
    "\n",
    "    if not any(word in assistant_response.lower() for word in keywords):\n",
    "        print(f\"Skipping {image_name}: caption does not include any of the keywords: {caption}\")\n",
    "        total_unidentified_images += 1\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"{image_name}: {caption}\")\n",
    "        total_identified_images += 1\n",
    "        os.makedirs(image_output_dir, exist_ok = True)\n",
    "\n",
    "    caption_ids = processor.tokenizer(caption, return_tensors=\"pt\").input_ids[0]\n",
    "    generated_token_ids = caption_ids.to(model.device)\n",
    "\n",
    "    target_layer = model.vision_tower.vision_model.encoder.layers[9].layer_norm2\n",
    "\n",
    "    # determine the target size for GradCAM's output\n",
    "    # this should be the H, W of the single view image input to the vision tower\n",
    "    _, _, target_h, target_w = pixel_values_for_gradcam.shape\n",
    "    gradcam_target_size = (target_w, target_h)\n",
    "\n",
    "    for i, token_id in enumerate(generated_token_ids):\n",
    "        # skip input prompt tokens, only consider generated tokens\n",
    "        if i < input_ids.shape[1]:\n",
    "            continue\n",
    "\n",
    "        token_str = processor.tokenizer.decode([token_id]).lower()\n",
    "        token_str_safe = \"\".join(c if c.isalnum() else \"_\" for c in token_str)\n",
    "        if token_str not in keywords:\n",
    "            continue\n",
    "\n",
    "        wrapped_base_inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "        # pass the expected num_image_views to the wrapper\n",
    "        wrapped = TokenLogitWrapper(model, wrapped_base_inputs, token_index=i, num_image_views=num_image_views_in_input)\n",
    "        torch.cuda.empty_cache()\n",
    "        cam = GradCAM(model=wrapped, target_layers=[target_layer], reshape_transform=reshape_transform)\n",
    "\n",
    "        # pass the single extracted view to GradCAM's input_tensor\n",
    "        grayscale = cam(input_tensor=pixel_values_for_gradcam)[0]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # get the heatmap for the image\n",
    "        vis_image = image.resize((grayscale.shape[1], grayscale.shape[0]))\n",
    "        rgb = np.array(vis_image).astype(np.float32) / 255.0\n",
    "        heatmap = plt.cm.jet(grayscale)[..., :3]\n",
    "        overlay = np.clip(0.5 * heatmap + 0.5 * rgb, 0, 1)\n",
    "        overlay_img = Image.fromarray(np.uint8(overlay * 255))\n",
    "\n",
    "        # get activations via the heatmap for the relevant people\n",
    "        for face_id, info in annotation_data[image_name].items():\n",
    "            face_rect = info[\"face_coords\"]\n",
    "            hair_path = os.path.join(hair_mask_dir, info[\"hair_mask_path\"])\n",
    "\n",
    "            if not os.path.exists(hair_path):\n",
    "                print(f\"Hair path doesn't exist ({hair_path}), continuing\")\n",
    "                continue\n",
    "\n",
    "            gender = info.get(\"gender\", \"unsure\")\n",
    "\n",
    "            hair_mask = np.load(hair_path)\n",
    "            hair_mask = (hair_mask > 0).astype(np.uint8)\n",
    "            hair_mask = cv2.resize(hair_mask, (grayscale.shape[1], grayscale.shape[0]), interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "            resized_bbox_face = scale_bbox(face_rect, orig_size, vis_image.size)\n",
    "            face_mask = rect_to_mask(grayscale.shape, resized_bbox_face)\n",
    "\n",
    "            # prevent overlap between hair and face\n",
    "            overlap = np.logical_and(face_mask, hair_mask).astype(np.uint8)\n",
    "            face_mask_clean = np.clip(face_mask - overlap, 0, 1)\n",
    "\n",
    "            # define background and combined masks\n",
    "            bg_mask = 1 - np.clip(hair_mask + face_mask_clean, 0, 1)\n",
    "            non_background_mask = np.clip(hair_mask + face_mask_clean, 0, 1)\n",
    "            \n",
    "            activations = {\n",
    "                \"hair\": average_activation(grayscale, hair_mask),\n",
    "                \"face\": average_activation(grayscale, face_mask_clean),\n",
    "                \"background\": average_activation(grayscale, bg_mask),\n",
    "                \"non_background\": average_activation(grayscale, non_background_mask)\n",
    "            }\n",
    "\n",
    "            total = sum(activations.values())\n",
    "            if total == 0:\n",
    "                print(f\"Skipping {image_name} due to zero regional activation\")\n",
    "                continue\n",
    "\n",
    "            activations = {k: v / total for k, v in activations.items()}\n",
    "\n",
    "            activation_log.append({\n",
    "                \"image\": image_name,\n",
    "                \"token_index\": i,\n",
    "                \"token\": token_str,\n",
    "                \"face_id\": face_id,\n",
    "                \"gender\": gender,\n",
    "                **activations\n",
    "            })\n",
    "\n",
    "            overlay_save_path = os.path.join(image_output_dir, f\"{i}_{token_str_safe}.png\")\n",
    "            overlay_img.save(overlay_save_path)\n",
    "\n",
    "            break  # only consider the first admissible token\n",
    "\n",
    "    processed_image_count += 1\n",
    "    if processed_image_count % SAVE_INTERVAL == 0:\n",
    "        save_batch_to_csv(activation_log, output_csv_path, csv_fieldnames, append=True)\n",
    "        activation_log.clear()\n",
    "\n",
    "if activation_log: # check if there's any data left in the log\n",
    "    save_batch_to_csv(activation_log, output_csv_path, csv_fieldnames, append=True)\n",
    "    activation_log.clear()\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
